{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7298add9-9107-4abd-a2d7-f79e4a17a95c",
   "metadata": {},
   "source": [
    "# GI67216 Jainam Maheshkumar Patel\n",
    "# week 8 Homework \n",
    "\n",
    "\n",
    "##### Study Reference: NLTK book, class notes and geeks for geeks...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5b883-7a66-46f8-b234-1781e67e1eea",
   "metadata": {},
   "source": [
    "#### For this line of code:\n",
    "\n",
    "##### dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "##### Rerun with no_above=.75, .9 and removed. How do the topics change?\n",
    "\n",
    "##### code is from the NLP.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9378fb00-3810-4408-b931-5f67ea140bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    with smart_open.open(url, \"rb\") as file:\n",
    "        with tarfile.open(fileobj=file) as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n",
    "                    member_bytes = tar.extractfile(member).read()\n",
    "                    yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f2df3bb-1e67-4bd1-9dd3-e526cdabecd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# here i only change Tokenize and preprocess the documents\n",
    "processed_docs = [simple_preprocess(doc) for doc in docs]\n",
    "\n",
    "# Create a dictionary representation of the processed documents\n",
    "dictionary = Dictionary(processed_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9683f1-72ca-48cf-bab4-17f5c2feb4ab",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cfb118f-4670-430a-b330-3a183e164bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter out words that occur less than 20 documents, or more than 75% of the documents\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd9c4634-fc81-49bc-9aba-e2a5018e1ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the processed documents into bag-of-words representation\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4ab281f-4424-4a08-8583-fcc494e32692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 7335\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c49a5c-f210-42ad-91b4-2cca98e1ff4d",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac6fe178-ffa5-494b-bf2d-732559ccd261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20, no_above=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae7ec906-46d1-45a9-b429-6e1c70dac6f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ecdb9d2-eb88-4bf1-a43a-d45241b8eebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 7335\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df57ffb-d363-4079-9f5b-7b6306af0482",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0039a9e2-c5ba-40ab-acc6-aca2950907e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8e75ed3-e39d-46cd-8ebd-d9255c4dd814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d50fbc79-e5aa-4877-96e7-40c064c58f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 7206\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78722e-939c-4f25-a630-9b54fc050909",
   "metadata": {},
   "source": [
    "###   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbe650-fb98-4695-9df9-73227de4d8e8",
   "metadata": {},
   "source": [
    "###   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112266f8-a7b8-4b22-ac9c-a0ad88089848",
   "metadata": {},
   "source": [
    "## For this line of code:\n",
    "\n",
    "#### Set training parameters.\n",
    "##### num_topics = 15\n",
    "##### Set to 10, 15, 20. Try to interpret the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320e98e-4bab-4454-8dd6-046317dae2e2",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffddcbd9-36d6-45aa-894b-2e772d2d37c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = 50  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a6f4a03-5edd-4759-9a58-af4e49cec83e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -0.8241.\n",
      "[([(0.013811877, 'error'), (0.01246848, 'training'), (0.011397988, 'units'), (0.0093870675, 'weights'), (0.009270743, 'hidden'), (0.009243075, 'weight'), (0.008202753, 'output'), (0.007356837, 'unit'), (0.0057177534, 'generalization'), (0.0051009455, 'layer'), (0.0043623247, 'patterns'), (0.0043271575, 'memory'), (0.0040424606, 'performance'), (0.004015429, 'order'), (0.0038884394, 'vector'), (0.0037954892, 'noise'), (0.003465954, 'pattern'), (0.0034419617, 'linear'), (0.003356877, 'net'), (0.0033248325, 'algorithm')], -0.583847973279933), ([(0.0098972125, 'algorithm'), (0.008403045, 'distribution'), (0.00619165, 'probability'), (0.006181076, 'gaussian'), (0.0054760003, 'parameters'), (0.0053779134, 'models'), (0.004692945, 'matrix'), (0.0044604754, 'density'), (0.004356607, 'mean'), (0.004356053, 'likelihood'), (0.003919856, 'log'), (0.0038918927, 'linear'), (0.0038538312, 'bayesian'), (0.0036907955, 'mixture'), (0.003687197, 'vector'), (0.003589883, 'estimate'), (0.0035291421, 'space'), (0.0035167725, 'approximation'), (0.0035136002, 'problem'), (0.0034589802, 'variables')], -0.7135507255809985), ([(0.010716633, 'algorithm'), (0.009446304, 'problem'), (0.0067607737, 'algorithms'), (0.00512856, 'search'), (0.004983305, 'optimization'), (0.0043586846, 'problems'), (0.004290788, 'system'), (0.0042683915, 'value'), (0.0042015365, 'local'), (0.004083416, 'method'), (0.0037114648, 'performance'), (0.003628954, 'rules'), (0.003438451, 'methods'), (0.0031668618, 'solution'), (0.0030773177, 'cost'), (0.0030746725, 'control'), (0.002867627, 'rule'), (0.002784991, 'approach'), (0.002778064, 'best'), (0.002757399, 'optimal')], -0.7181788189287737), ([(0.009555822, 'neurons'), (0.0087902, 'cells'), (0.0072902935, 'cell'), (0.0064213066, 'activity'), (0.006194644, 'neuron'), (0.005620191, 'visual'), (0.005334783, 'synaptic'), (0.0052541937, 'response'), (0.0047826143, 'stimulus'), (0.0046625887, 'firing'), (0.0045646713, 'cortex'), (0.0042629973, 'spike'), (0.0039048858, 'system'), (0.0036344328, 'cortical'), (0.0036033439, 'were'), (0.003452464, 'connections'), (0.003282583, 'direction'), (0.0031973922, 'al'), (0.0031661673, 'et'), (0.0031168635, 'fig')], -0.7439300361965102), ([(0.015269304, 'training'), (0.008855481, 'recognition'), (0.0077399774, 'speech'), (0.0076408875, 'were'), (0.006891439, 'output'), (0.0066662463, 'hidden'), (0.006156221, 'performance'), (0.0057306485, 'system'), (0.0054171826, 'units'), (0.005257806, 'trained'), (0.0052247345, 'word'), (0.0052124048, 'layer'), (0.005163446, 'classification'), (0.004480363, 'test'), (0.0041137375, 'error'), (0.003930851, 'classifier'), (0.0038009987, 'net'), (0.003715923, 'context'), (0.0036530162, 'models'), (0.003518555, 'features')], -0.768433289732076), ([(0.01018886, 'functions'), (0.00730151, 'node'), (0.0069078, 'any'), (0.0066271406, 'nodes'), (0.0063777566, 'theorem'), (0.0063047973, 'tree'), (0.0060736705, 'threshold'), (0.0058038, 'let'), (0.0047970023, 'bound'), (0.004420124, 'class'), (0.0044188676, 'size'), (0.004367212, 'probability'), (0.0040556346, 'weights'), (0.003972898, 'proof'), (0.0035918395, 'bounds'), (0.0034478942, 'dimension'), (0.003322505, 'layer'), (0.0033012878, 'result'), (0.0032750897, 'following'), (0.003262082, 'output')], -0.849843950670075), ([(0.011395605, 'training'), (0.0076862327, 'kernel'), (0.0066637285, 'error'), (0.0059825536, 'linear'), (0.0056603896, 'vector'), (0.0052319923, 'support'), (0.005097566, 'algorithm'), (0.004919028, 'test'), (0.004754308, 'feature'), (0.004684324, 'problem'), (0.004619541, 'examples'), (0.0045084166, 'space'), (0.0040604966, 'performance'), (0.0039301286, 'functions'), (0.0038371913, 'regression'), (0.003699947, 'method'), (0.003564837, 'sets'), (0.0035592085, 'classification'), (0.0035351217, 'margin'), (0.0034457543, 'noise')], -0.9034254172141203), ([(0.017991368, 'image'), (0.011986451, 'images'), (0.007435956, 'object'), (0.006477097, 'feature'), (0.0063563767, 'recognition'), (0.0060828268, 'visual'), (0.005590974, 'features'), (0.004916889, 'distance'), (0.0044740196, 'objects'), (0.004361315, 'were'), (0.004219668, 'space'), (0.0040178834, 'local'), (0.0039611477, 'map'), (0.0038968837, 'representation'), (0.0037485384, 'face'), (0.0036444368, 'system'), (0.0034022443, 'human'), (0.0033994142, 'vision'), (0.0032481682, 'pixel'), (0.0030677684, 'pattern')], -0.9056972864908526), ([(0.008666542, 'signal'), (0.007919787, 'system'), (0.007851266, 'analog'), (0.007822477, 'output'), (0.007381024, 'chip'), (0.00721186, 'circuit'), (0.0060993624, 'neuron'), (0.0053614713, 'noise'), (0.0052333637, 'current'), (0.0049581565, 'voltage'), (0.0044242805, 'frequency'), (0.0042825155, 'processing'), (0.0041203597, 'vlsi'), (0.004100509, 'signals'), (0.0035130952, 'filter'), (0.003409667, 'neurons'), (0.0030470332, 'shows'), (0.0029958042, 'channel'), (0.0029754024, 'fig'), (0.0029088017, 'implementation')], -1.0177916394808428), ([(0.021868745, 'state'), (0.010031641, 'control'), (0.0071323407, 'system'), (0.0067012, 'states'), (0.0060741957, 'value'), (0.0057006236, 'action'), (0.005421263, 'reinforcement'), (0.005057199, 'policy'), (0.0039916043, 'robot'), (0.0039466675, 'space'), (0.0039055652, 'task'), (0.0038433874, 'dynamic'), (0.003815771, 'optimal'), (0.0036902088, 'algorithm'), (0.0035905063, 'trajectory'), (0.003519361, 'step'), (0.0035109352, 'controller'), (0.0033719249, 'dynamics'), (0.0033679127, 'actions'), (0.003178259, 'problem')], -1.036357571737257)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936f6ea-7ab8-46f6-be96-63088a12e328",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4ec260c-c29e-4226-8495-d33c303a3e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 15\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = 50  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34919f6e-5345-4fcd-8e90-3a40ada2c1bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.1864.\n",
      "[([(0.01484111, 'gaussian'), (0.008187009, 'likelihood'), (0.0077049327, 'bayesian'), (0.007562267, 'density'), (0.0073546804, 'mixture'), (0.0068037463, 'prior'), (0.006628812, 'posterior'), (0.006019761, 'log'), (0.005681375, 'noise'), (0.005618973, 'matrix'), (0.005116597, 'variance'), (0.0050187213, 'hidden'), (0.0046867793, 'covariance'), (0.0042629526, 'component'), (0.0042575323, 'components'), (0.004206298, 'estimate'), (0.0041464083, 'estimation'), (0.0041410103, 'variables'), (0.003932032, 'em'), (0.00385527, 'approximation')], -0.8560382384456342), ([(0.018856067, 'units'), (0.012363926, 'unit'), (0.012168009, 'memory'), (0.010466023, 'hidden'), (0.008779003, 'recurrent'), (0.0078038652, 'sequence'), (0.005929934, 'states'), (0.005764493, 'sequences'), (0.0056143254, 'patterns'), (0.005505441, 'activation'), (0.0050339573, 'representations'), (0.0048505496, 'structure'), (0.0044589574, 'net'), (0.004250602, 'representation'), (0.004122082, 'context'), (0.0038003882, 'architecture'), (0.0037373868, 'connectionist'), (0.0035167544, 'distributed'), (0.0034436502, 'layer'), (0.0033683095, 'node')], -0.956602466523573), ([(0.014725114, 'neurons'), (0.0122916205, 'neuron'), (0.009713855, 'cell'), (0.009359317, 'synaptic'), (0.008616534, 'spike'), (0.008467732, 'firing'), (0.007906441, 'cells'), (0.0071081957, 'response'), (0.0070244856, 'activity'), (0.006334696, 'frequency'), (0.0058959383, 'stimulus'), (0.0051472248, 'signal'), (0.004684305, 'synapses'), (0.004414067, 'cortex'), (0.0042426256, 'fig'), (0.0040866635, 'excitatory'), (0.004042455, 'noise'), (0.00402267, 'inhibitory'), (0.0039204196, 'potential'), (0.0038328615, 'membrane')], -0.9779549513455541), ([(0.007576563, 'matrix'), (0.0070689977, 'weight'), (0.007038271, 'dynamics'), (0.0068326173, 'noise'), (0.0067372182, 'gradient'), (0.0061993403, 'equation'), (0.004799305, 'equations'), (0.004796601, 'eq'), (0.004777148, 'convergence'), (0.0046172203, 'optimal'), (0.0042862366, 'neurons'), (0.0041740593, 'term'), (0.0038771448, 'solution'), (0.0037483308, 'generalization'), (0.003660903, 'neuron'), (0.0036241463, 'parameter'), (0.0035757553, 'rule'), (0.0034381456, 'nonlinear'), (0.003397588, 'stochastic'), (0.0033113952, 'descent')], -1.0012734570337327), ([(0.010759983, 'hidden'), (0.00856673, 'units'), (0.007947431, 'layer'), (0.007756188, 'classifier'), (0.0077402713, 'classification'), (0.006668314, 'trained'), (0.005664044, 'net'), (0.0052300417, 'classifiers'), (0.005188843, 'generalization'), (0.005077979, 'class'), (0.0050087273, 'weight'), (0.0047397898, 'table'), (0.004628066, 'prediction'), (0.0045102565, 'decision'), (0.004328985, 'nodes'), (0.0042909975, 'tree'), (0.0042755282, 'unit'), (0.004202123, 'node'), (0.00407261, 'examples'), (0.004050052, 'task')], -1.0234372766452673), ([(0.008826953, 'weight'), (0.008148487, 'analog'), (0.008037979, 'chip'), (0.0059610084, 'control'), (0.0054271217, 'circuit'), (0.0053688167, 'neuron'), (0.0045586075, 'parallel'), (0.004555464, 'memory'), (0.0042229434, 'implementation'), (0.003979446, 'bit'), (0.0038446148, 'vlsi'), (0.0037612307, 'digital'), (0.003606976, 'design'), (0.0034378155, 'units'), (0.003378266, 'hardware'), (0.0033465298, 'synapse'), (0.0032813807, 'layer'), (0.0031399895, 'architecture'), (0.0029941457, 'speed'), (0.0029062089, 'net')], -1.0537666716441756), ([(0.021810608, 'image'), (0.017553886, 'images'), (0.011691209, 'recognition'), (0.011540486, 'object'), (0.009621247, 'features'), (0.008138379, 'feature'), (0.0078585725, 'face'), (0.0064451457, 'objects'), (0.0058447607, 'representation'), (0.005196141, 'rules'), (0.005088426, 'classification'), (0.0038580599, 'faces'), (0.0037390909, 'rule'), (0.0036407306, 'layer'), (0.0036182823, 'scale'), (0.0035489309, 'class'), (0.003506839, 'examples'), (0.0034701112, 'pixel'), (0.003441142, 'view'), (0.0034358373, 'threshold')], -1.1105804598495075), ([(0.010984792, 'visual'), (0.010932857, 'motion'), (0.01052075, 'image'), (0.0077412245, 'field'), (0.0067813974, 'spatial'), (0.006647546, 'filter'), (0.005998751, 'circuit'), (0.0056339055, 'receptive'), (0.0051618307, 'response'), (0.004677055, 'filters'), (0.004585807, 'images'), (0.0045675994, 'cells'), (0.0045543597, 'contrast'), (0.0044443347, 'frequency'), (0.0044006496, 'vision'), (0.004391029, 'velocity'), (0.0042979536, 'analog'), (0.0042960583, 'signal'), (0.004205489, 'orientation'), (0.0038093866, 'direction')], -1.1558426500447045), ([(0.010589673, 'policy'), (0.009884754, 'action'), (0.009484985, 'reinforcement'), (0.008907163, 'optimal'), (0.007471509, 'control'), (0.006618288, 'states'), (0.0060274145, 'actions'), (0.0056990585, 'algorithms'), (0.004534477, 'reward'), (0.0045090867, 'dynamic'), (0.0043084593, 'goal'), (0.00422299, 'sutton'), (0.0042056073, 'td'), (0.004088485, 'task'), (0.0037378112, 'decision'), (0.0036905194, 'robot'), (0.003606943, 'agent'), (0.0035696367, 'markov'), (0.0034581309, 'convergence'), (0.0033577047, 'stochastic')], -1.1611852484117302), ([(0.008011787, 'let'), (0.0071309158, 'theorem'), (0.006908075, 'examples'), (0.006319723, 'class'), (0.0062107844, 'bound'), (0.0052616904, 'xi'), (0.004780693, 'approximation'), (0.0047430387, 'bounds'), (0.0045188833, 'loss'), (0.004365533, 'threshold'), (0.0043321927, 'dimension'), (0.004228625, 'sample'), (0.004163463, 'complexity'), (0.003780835, 'log'), (0.0037230565, 'support'), (0.003693123, 'proof'), (0.0036261294, 'generalization'), (0.0035228503, 'polynomial'), (0.0035058074, 'kernel'), (0.0034337628, 'points')], -1.2215107762697701), ([(0.02205249, 'speech'), (0.020943282, 'recognition'), (0.013813472, 'word'), (0.0069274846, 'words'), (0.006743363, 'context'), (0.0065479786, 'hmm'), (0.006389109, 'hidden'), (0.0058213626, 'speaker'), (0.0057001323, 'trained'), (0.0056394525, 'layer'), (0.004439851, 'net'), (0.004419814, 'character'), (0.004288896, 'units'), (0.0041755484, 'signal'), (0.004129156, 'acoustic'), (0.0041038436, 'segmentation'), (0.0038920778, 'characters'), (0.0038772854, 'phoneme'), (0.0038075135, 'features'), (0.0037291737, 'architecture')], -1.263615619080291), ([(0.010140037, 'cells'), (0.009983771, 'units'), (0.009241293, 'map'), (0.008279689, 'eye'), (0.008270895, 'visual'), (0.0073966007, 'direction'), (0.0073150774, 'layer'), (0.006807758, 'activity'), (0.0064701545, 'unit'), (0.0061393464, 'orientation'), (0.0058078826, 'head'), (0.00549067, 'cortex'), (0.005113121, 'connections'), (0.0049656313, 'maps'), (0.004675541, 'cortical'), (0.0044255136, 'motion'), (0.00440179, 'cell'), (0.0042580925, 'neurons'), (0.004182503, 'lateral'), (0.0041023754, 'ocular')], -1.3724449694264231), ([(0.0075380285, 'variables'), (0.0071620448, 'em'), (0.0067724013, 'clustering'), (0.00628616, 'graph'), (0.0058830227, 'experts'), (0.004853796, 'mixture'), (0.004848624, 'xi'), (0.004682484, 'conditional'), (0.004658475, 'cluster'), (0.004239314, 'structure'), (0.0042074416, 'likelihood'), (0.003755356, 'tree'), (0.0037343788, 'estimation'), (0.0037069025, 'missing'), (0.003682714, 'optimization'), (0.0036801891, 'matching'), (0.0036449675, 'annealing'), (0.0034788677, 'level'), (0.0034229753, 'jordan'), (0.0032367664, 'nodes')], -1.4664909181409966), ([(0.018538475, 'control'), (0.012538897, 'motor'), (0.008443058, 'position'), (0.008002617, 'field'), (0.0073659257, 'hand'), (0.0072186, 'human'), (0.00721761, 'visual'), (0.006812655, 'target'), (0.00676159, 'object'), (0.006691589, 'movement'), (0.006608331, 'trajectory'), (0.0065271156, 'arm'), (0.006463017, 'subjects'), (0.00617951, 'task'), (0.00600594, 'forward'), (0.005893115, 'movements'), (0.005102483, 'feedback'), (0.0050206017, 'inverse'), (0.004832938, 'controller'), (0.0045229224, 'dynamics')], -1.4845718435151047), ([(0.01331808, 'distance'), (0.009720221, 'vectors'), (0.0066316114, 'code'), (0.006063569, 'tangent'), (0.0059771272, 'dimensional'), (0.0056104567, 'points'), (0.0053566173, 'feature'), (0.004835036, 'image'), (0.0046377345, 'ica'), (0.0046374653, 'search'), (0.004547661, 'structure'), (0.004452894, 'algorithms'), (0.0041667926, 'basis'), (0.0040101903, 'nearest'), (0.0038054753, 'similarity'), (0.0035552315, 'coding'), (0.003528199, 'genetic'), (0.0034990131, 'class'), (0.0034008329, 'features'), (0.003324947, 'distances')], -1.6913676937402855)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29dab8-6b5d-4156-87ac-f5007be1ffa7",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7889f348-3962-482d-afdb-9351a31f7b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 20\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = 50  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08f9401d-6331-4b0d-a2fe-439fcaa7d603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.3724.\n",
      "[([(0.012322334, 'activity'), (0.012304297, 'cells'), (0.009501599, 'neurons'), (0.0088276835, 'cell'), (0.008624201, 'cortex'), (0.008460631, 'synaptic'), (0.007532004, 'cortical'), (0.0074298847, 'connections'), (0.0060426053, 'orientation'), (0.0060149343, 'patterns'), (0.005378238, 'excitatory'), (0.0051118867, 'inhibitory'), (0.0050860425, 'visual'), (0.004734932, 'phase'), (0.0043327184, 'layer'), (0.004280559, 'neuron'), (0.0039127963, 'lateral'), (0.0039093094, 'synapses'), (0.0039088456, 'fig'), (0.0038086695, 'response')], -0.9098136048320419), ([(0.023259625, 'image'), (0.01648167, 'images'), (0.011230145, 'recognition'), (0.010063705, 'feature'), (0.010026527, 'object'), (0.009532376, 'features'), (0.0059831943, 'objects'), (0.0051668487, 'face'), (0.0051355734, 'visual'), (0.004938845, 'representation'), (0.0044822465, 'pixel'), (0.0037796465, 'vision'), (0.003716603, 'scale'), (0.0035707199, 'pixels'), (0.0033211198, 'human'), (0.0032412102, 'view'), (0.002978628, 'level'), (0.0029734543, 'layer'), (0.002906703, 'dimensional'), (0.0028905638, 'hand')], -0.9194460290393542), ([(0.011621563, 'gaussian'), (0.009281392, 'matrix'), (0.006963783, 'noise'), (0.0054912097, 'likelihood'), (0.0050441814, 'approximation'), (0.0048806434, 'posterior'), (0.004763182, 'gradient'), (0.004657583, 'convergence'), (0.0045460314, 'stochastic'), (0.00442873, 'covariance'), (0.004362031, 'bayesian'), (0.0042984323, 'log'), (0.004277135, 'density'), (0.0042059487, 'variables'), (0.0040733605, 'em'), (0.0039613387, 'component'), (0.0038113468, 'prior'), (0.0037544, 'variance'), (0.0037518716, 'algorithms'), (0.0037476902, 'components')], -0.9652859120650936), ([(0.0247629, 'units'), (0.019760251, 'hidden'), (0.015285418, 'unit'), (0.014450643, 'layer'), (0.008549016, 'node'), (0.008226163, 'nodes'), (0.008186528, 'patterns'), (0.0062867934, 'net'), (0.006113534, 'classifier'), (0.0061114393, 'propagation'), (0.0057313424, 'back'), (0.0057011824, 'trained'), (0.00455014, 'weight'), (0.004305775, 'classification'), (0.0042664586, 'activation'), (0.0041802186, 'classifiers'), (0.0038364187, 'task'), (0.0038062863, 'architecture'), (0.0033065297, 'generalization'), (0.0030781648, 'representations')], -0.9725101842974362), ([(0.012299873, 'chip'), (0.012212182, 'analog'), (0.010272302, 'circuit'), (0.009814745, 'neuron'), (0.0067693517, 'weight'), (0.0065454445, 'vlsi'), (0.005978622, 'neurons'), (0.005682586, 'voltage'), (0.005150759, 'bit'), (0.0051283943, 'memory'), (0.005016172, 'implementation'), (0.004601484, 'parallel'), (0.0044804285, 'circuits'), (0.0043361695, 'digital'), (0.0042727166, 'design'), (0.004011318, 'synapse'), (0.0039419015, 'hardware'), (0.0034559295, 'pp'), (0.0034195033, 'fig'), (0.003158068, 'signal')], -1.0094967867058333), ([(0.015472717, 'visual'), (0.014141821, 'motion'), (0.010631917, 'response'), (0.009057711, 'stimulus'), (0.007814206, 'spatial'), (0.0077285315, 'direction'), (0.007254515, 'eye'), (0.006521255, 'field'), (0.0061690863, 'velocity'), (0.0056308005, 'cells'), (0.0056058634, 'stimuli'), (0.005086289, 'responses'), (0.0050436864, 'temporal'), (0.0049775844, 'units'), (0.0048670764, 'receptive'), (0.0047768904, 'position'), (0.0045843343, 'neurons'), (0.0043993294, 'contrast'), (0.0043656235, 'frequency'), (0.0038272834, 'attention')], -1.053376015158446), ([(0.0133289965, 'weight'), (0.012117845, 'units'), (0.010911471, 'hidden'), (0.0075525553, 'net'), (0.0066075833, 'unit'), (0.0064616282, 'sequence'), (0.0064464873, 'term'), (0.0062500387, 'prediction'), (0.0057829954, 'threshold'), (0.0057516266, 'recurrent'), (0.0051365998, 'series'), (0.0047254586, 'layer'), (0.003977986, 'nets'), (0.0038906543, 'complexity'), (0.0038616199, 'gradient'), (0.003790021, 'sequences'), (0.0034218493, 'architecture'), (0.0032378915, 'noise'), (0.0032073248, 'feedforward'), (0.003162037, 'activation')], -1.0895582529821697), ([(0.0135835735, 'control'), (0.013077948, 'policy'), (0.0116518065, 'action'), (0.011540092, 'reinforcement'), (0.0096620405, 'optimal'), (0.007841714, 'states'), (0.007231559, 'actions'), (0.005870802, 'dynamic'), (0.0055040955, 'controller'), (0.005423923, 'reward'), (0.005054315, 'sutton'), (0.004982929, 'td'), (0.0046233665, 'decision'), (0.0044898205, 'algorithms'), (0.0041116583, 'programming'), (0.0040996415, 'agent'), (0.0037100015, 'cost'), (0.0035591295, 'rl'), (0.0034741592, 'environment'), (0.0034599234, 'task')], -1.1621347427850446), ([(0.013500879, 'dynamics'), (0.007961218, 'neurons'), (0.006447532, 'neuron'), (0.0061817993, 'teacher'), (0.0061360477, 'generalization'), (0.005982601, 'student'), (0.0057890736, 'limit'), (0.005739695, 'equations'), (0.0057241526, 'capacity'), (0.00532573, 'phase'), (0.005001817, 'equation'), (0.0045818416, 'symmetric'), (0.0045255986, 'fig'), (0.0043596704, 'line'), (0.0043347906, 'stable'), (0.00432753, 'rule'), (0.004272108, 'parameter'), (0.004165718, 'phys'), (0.004084544, 'patterns'), (0.0040607885, 'noise')], -1.1663108742548967), ([(0.021195533, 'spike'), (0.016236605, 'neuron'), (0.015143463, 'firing'), (0.014053262, 'neurons'), (0.012899346, 'noise'), (0.010011738, 'signal'), (0.00788316, 'spikes'), (0.007749286, 'potential'), (0.00752329, 'membrane'), (0.005871267, 'synaptic'), (0.0058189263, 'response'), (0.0054632183, 'voltage'), (0.00540911, 'threshold'), (0.005140628, 'frequency'), (0.0048085297, 'cell'), (0.0047023315, 'coding'), (0.0045736125, 'temporal'), (0.0045150756, 'ms'), (0.0045126635, 'spiking'), (0.0044091023, 'channel')], -1.1719249368363622), ([(0.008710142, 'mixture'), (0.0063982625, 'class'), (0.00626576, 'likelihood'), (0.005919252, 'density'), (0.005690827, 'prediction'), (0.005302899, 'bayesian'), (0.005120529, 'regression'), (0.00506932, 'estimate'), (0.0050639515, 'prior'), (0.0046601417, 'variance'), (0.004478446, 'log'), (0.004459001, 'tree'), (0.0044253953, 'gaussian'), (0.0042219884, 'sample'), (0.004219983, 'probabilities'), (0.0041861213, 'em'), (0.0041602016, 'validation'), (0.0041080276, 'classification'), (0.0041019158, 'experts'), (0.0040183067, 'conditional')], -1.1795559463345893), ([(0.027519489, 'recognition'), (0.02663776, 'speech'), (0.01248151, 'hmm'), (0.0110853575, 'speaker'), (0.006155991, 'hybrid'), (0.005148384, 'experiments'), (0.0051220777, 'word'), (0.004977214, 'character'), (0.0049711596, 'trained'), (0.004357537, 'acoustic'), (0.004058674, 'hidden'), (0.004049949, 'context'), (0.0040367125, 'markov'), (0.0040360587, 'sequence'), (0.0040359246, 'architecture'), (0.0040132133, 'recognizer'), (0.00394193, 'multi'), (0.0039055422, 'hmms'), (0.0037841778, 'probabilities'), (0.003702748, 'feature')], -1.1824395245230461), ([(0.008240568, 'let'), (0.0075934385, 'class'), (0.0073939185, 'theorem'), (0.0062693157, 'examples'), (0.005985259, 'xi'), (0.005693516, 'distance'), (0.00547124, 'bound'), (0.0053516547, 'approximation'), (0.005200159, 'vectors'), (0.0050035873, 'points'), (0.004699601, 'kernel'), (0.0046673324, 'dimension'), (0.004577047, 'bounds'), (0.0043674083, 'algorithms'), (0.0042335405, 'loss'), (0.0040527624, 'support'), (0.0038061177, 'optimal'), (0.0035633175, 'sample'), (0.0032736212, 'tangent'), (0.0032259848, 'classification')], -1.235071715547742), ([(0.00653318, 'energy'), (0.0063379314, 'solution'), (0.0063143275, 'points'), (0.0061839405, 'field'), (0.006174478, 'optimization'), (0.0048187464, 'equations'), (0.0046582613, 'recurrent'), (0.004589721, 'solutions'), (0.004300374, 'surface'), (0.0042411014, 'constraint'), (0.0041577737, 'equation'), (0.004091959, 'states'), (0.0040408857, 'attractor'), (0.0039874767, 'constraints'), (0.0039795088, 'initial'), (0.0037364927, 'dynamics'), (0.0035737974, 'attractors'), (0.0035079382, 'matrix'), (0.0034458057, 'gradient'), (0.0034144602, 'net')], -1.2413833058714168), ([(0.019301325, 'control'), (0.012697243, 'robot'), (0.011468319, 'motor'), (0.010179208, 'trajectory'), (0.008771937, 'arm'), (0.007173449, 'forward'), (0.006695158, 'position'), (0.0064723627, 'task'), (0.006391412, 'movement'), (0.0063662413, 'hand'), (0.0057613715, 'controller'), (0.0056414683, 'inverse'), (0.005193219, 'joint'), (0.0051855114, 'dynamics'), (0.0049057673, 'feedback'), (0.004901534, 'goal'), (0.0046325983, 'trajectories'), (0.0043415288, 'desired'), (0.0041041803, 'movements'), (0.004030523, 'target')], -1.2699594270591874), ([(0.017925894, 'rules'), (0.016273934, 'memory'), (0.015124704, 'rule'), (0.0066385996, 'examples'), (0.006210822, 'representation'), (0.005713867, 'knowledge'), (0.004911413, 'tree'), (0.004769551, 'representations'), (0.0042870077, 'learned'), (0.0041190363, 'structure'), (0.0040003485, 'similarity'), (0.003908149, 'table'), (0.0038541702, 'connectionist'), (0.003673693, 'features'), (0.0036201486, 'nearest'), (0.0034947759, 'class'), (0.003384511, 'trees'), (0.0032980961, 'symbolic'), (0.0032157542, 'domain'), (0.003151838, 'search')], -1.396198740206907), ([(0.033369336, 'word'), (0.022520807, 'speech'), (0.018036513, 'words'), (0.016384894, 'context'), (0.012285835, 'recognition'), (0.008373083, 'phoneme'), (0.008128184, 'mlp'), (0.0075882277, 'gamma'), (0.0073290938, 'sequence'), (0.007300486, 'ensemble'), (0.006820895, 'phonetic'), (0.0062817032, 'layer'), (0.0062232185, 'frame'), (0.0060475687, 'signal'), (0.005898661, 'letter'), (0.0056861364, 'vowel'), (0.005405244, 'trained'), (0.005328391, 'hidden'), (0.005218396, 'ms'), (0.0052018967, 'frequency')], -1.5902944298470874), ([(0.017339086, 'cell'), (0.014976175, 'cells'), (0.01172174, 'clustering'), (0.01096402, 'cluster'), (0.0099351425, 'head'), (0.009107471, 'direction'), (0.0069106477, 'clusters'), (0.006458792, 'rat'), (0.005668487, 'flow'), (0.0046005305, 'region'), (0.0045435065, 'receptor'), (0.0042674295, 'fig'), (0.0042534815, 'chain'), (0.004242262, 'place'), (0.004178243, 'dendritic'), (0.004095154, 'neurons'), (0.0038252892, 'rotation'), (0.003633289, 'noise'), (0.0035862422, 'brain'), (0.0034357184, 'distance')], -1.6761319589528716), ([(0.013601547, 'signal'), (0.009463402, 'auditory'), (0.009187236, 'sound'), (0.008799978, 'frequency'), (0.00802369, 'signals'), (0.0072362134, 'channel'), (0.006586589, 'spectral'), (0.0052775927, 'classification'), (0.0052076736, 'noise'), (0.005031248, 'source'), (0.0050296765, 'detection'), (0.004873595, 'template'), (0.004499007, 'channels'), (0.004249644, 'localization'), (0.0040217363, 'target'), (0.003893242, 'correlation'), (0.0038654811, 'game'), (0.0037357023, 'cochlear'), (0.0034706148, 'sources'), (0.0034057621, 'sounds')], -2.3372658374748245), ([(0.00950198, 'variables'), (0.007080922, 'query'), (0.0068525984, 'xi'), (0.0059629353, 'graph'), (0.005940938, 'let'), (0.005662447, 'perceptron'), (0.005198114, 'np'), (0.0050563454, 'queries'), (0.0048721973, 'variable'), (0.004715698, 'examples'), (0.004556294, 'generalization'), (0.0045234514, 'loss'), (0.004413406, 'weight'), (0.004252807, 'disparity'), (0.0040718294, 'wt'), (0.003948495, 'documents'), (0.0037782986, 'perceptrons'), (0.0036559384, 'belief'), (0.0035583244, 'song'), (0.0034972029, 'stopping')], -3.919950740641432)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22533e-bd63-48f9-8746-9a3ca6238a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397f1a92-3e18-4296-b56c-3f4e6aec176f",
   "metadata": {},
   "source": [
    "#### the changes which i see after change the values are listed below:\n",
    "\n",
    "- by changing no_above from 0.5 to 0.75 or 0.9,result in a more refined set of topics because it filters out more common terms that may not contribute significantly to distinguishing topics and 0.75 ,0.9 gave same results \n",
    "\n",
    "- after removing no_above entirely lead to including more common terms, potentially resulting in broader and less distinct topics.\n",
    "\n",
    "- by decreasing num_topics from 15 to 10  lead to more general and overlapping topics, as the model is forced to group more documents into more distinct topics.\n",
    "\n",
    "- and by increasing num_topics to 20 lead to more specific and narrowly focused topics, as the model can create not more distinct clusters of terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab83c34-377e-4b9f-96eb-af7815602da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
